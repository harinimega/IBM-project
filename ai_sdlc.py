# -*- coding: utf-8 -*-
"""AI - SDLC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hl3DydPNiD22NeLolUkeVUcGe4_c-HBx
"""

import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_name="ibm-granite/granite-3.2-2b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model=AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None # Set device_map to None when CUDA is not availabl
)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

def generate_response (prompt, max_length=512):
    try:
        inputs = tokenizer (prompt, return_tensors="pt", truncation=True, max_length=512)
        #if torch.cuda.is_available(): # Remove this as device_map handles device placement
        # inputs =(k: v.to (model.device) for k, v in inputs.items())
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=max_length,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        response = tokenizer.decode (outputs [0], skip_special_tokens=True)
        response = response.replace(prompt, "").strip()
        print (f"Generated response: {response}") # Add print statement
        return response
    except Exception as e:
        print(f"Error during response generation: {e}") # Add error handling
        return f"Error generating response: {e}"

def concept_explanation(concept):
    prompt = f"Explain the concept of {concept} in detail with example"
    print(f"Concept explanation prompt: {prompt}") # Add print statement
    return generate_response (prompt, max_length=800)

def quiz_generator(concept):
    prompt = f"Generate 5 quiz question about {concept} with different quiz types (multiple choice, true/false, short answer):, Give me the answers at the end:"
    print(f"Quiz generator prompt: {prompt}") # Add print statement
    return generate_response (prompt, max_length=1200)

#create gradio
with gr.Blocks() as app:
    gr.Markdown("# Educational AI assistant")
    with gr.Tabs():
        with gr.TabItem("Concept Explaination"):
            concept_input = gr. Textbox(label="Enter a concept", placeholder="e.g., Machine learning")
            explain_btn = gr.Button("Explain")
            explanation_output = gr. Textbox (label="Explaination", lines=10)
            explain_btn.click(concept_explanation, inputs = concept_input, outputs = explanation_output)
        with gr.TabItem("Quiz Generator"):
            quiz_input = gr.Textbox (label="Enter a topic", placeholder="e.g., Machine Learning")
            quiz_btn = gr.Button("Generate quiz")
            quiz_output = gr.Textbox (label="Quiz_Question & Answers", lines=15)
            quiz_btn.click(quiz_generator, inputs = quiz_input, outputs = quiz_output)

app.launch()